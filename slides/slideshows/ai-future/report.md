The PolicyEngine team concludes the event with a live demonstration of their multi-agent AI research workflow, recently tested on policy analysis tasks similar to their UK carbon dividend study. Using Claude Code's multi-agent system, PolicyEngine has configured specialized agents that automate different parts of the policy analysis pipeline: a data fetching agent that accesses PolicyEngine microsimulation data from GitHub repositories and documentation, a script writer that generates Python code using the policyengine-uk package, and a report generator that produces formatted research output with Plotly visualisations. The speakers demonstrate how Claude Code automatically routes tasks to appropriate agents based on natural language requests, showing real-time policy analysis without manual coordination between components.

The presentation addresses both successes and limitations discovered during testing. The team found that multi-agent workflows excel at standard distributional analyses—calculating poverty rates, Gini coefficients, and decile-level impacts—saving significant time on repetitive tasks while maintaining PolicyEngine's house style. However, they reveal challenges with complex benefit program interactions, the need for precise prompting rather than generic instructions, and the importance of specialized reviewer agents for meaningful code critique. The speakers announce the forthcoming policyengine-claude plugin, which packages these insights for other researchers using Claude Code with PolicyEngine, incorporating Anthropic's Skills ecosystem for common policy analyses and direct database access through the Model Context Protocol. They emphasise the right division of labour: AI handles routine policy updates with constant analysis structures, while human researchers focus on questions requiring genuine policy expertise, complex methodological choices, and political context—ensuring that advancing AI serves to augment rather than replace expert judgment in evidence-based policymaking.